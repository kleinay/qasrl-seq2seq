{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any, Dict, Callable, Iterable\n",
    "import json, os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import qanom\n",
    "from qanom.annotations.common import read_annot_csv\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "def plot_bar(labels, array1, *args):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar(labels, array1)\n",
    "    ax.bar(labels, args[0], color='g')\n",
    "    plt.show()\n",
    "    # X = np.arange(4)\n",
    "    # fig = plt.figure()\n",
    "    # ax = fig.add_axes([0,0,1,1])\n",
    "    # ax.bar(X + 0.00, array1, color = 'b', width = 0.25)\n",
    "    # ax.bar(X + 0.25, data[1], color = 'g', width = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: qanom/default\n",
      "Reusing dataset qanom (/home/nlp/kleinay/.cache/huggingface/datasets/biu-nlp___qanom/default/1.1.0/44d54349c6d3f70e326208bf63485003c5410d38a6aae87eb80d74cf887627d0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c4d637e21c43709c1af2856211cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare Helper Function: Segregate evaluation by column value\n",
    "qanom_dataset = datasets.load_dataset(\"biu-nlp/qanom\")\n",
    "qanom_test_df = pd.DataFrame(qanom_dataset[\"test\"])                              \n",
    "from evaluation import run_qanom_evaluation\n",
    "\n",
    "def evaluate_precision_by_column(predictions_df, column_name, take=None):\n",
    "    values = predictions_df[column_name].unique().tolist()\n",
    "    if take is not None:\n",
    "        values = set(values) & set(take)\n",
    "    eval_per_val = {} # {value : (UA precision, LA precision)}\n",
    "    for val in values:\n",
    "        part_pred_df = predictions_df[predictions_df[column_name]==val].copy()\n",
    "        print(f\"Evaluating for {column_name} == '{val}': (Notice that recall might be un-informative!)\")\n",
    "        eval_results = run_qanom_evaluation(part_pred_df, qanom_test_df.copy())\n",
    "        print(eval_results[:2], \"\\n\")  \n",
    "        eval_per_val[val] = (eval_results[0].prec(), eval_results[1].prec())\n",
    "    return eval_per_val  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring QA-level Condfidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kleinay/qanom-seq2seq-model-joint\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kleinay/qanom-seq2seq-model-joint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = \"parse: yesterday , you<extra_id_10> took<extra_id_10> my hand with your fork to ask me out .<extra_id_1> took\"\n",
    "input_ids = tokenizer(input_seq, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "# decoder_input_ids = tokenizer(\"<pad> when did someone take something _ _?<extra_id_7> yesterday</s>\",  \n",
    "#                               add_special_tokens=False, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> when did someone take something _ _?<extra_id_7> yesterday<extra_id_9> who _ _ took something _ _?<extra_id_7> you<extra_id_9> what did someone take _ _ _?<extra_id_7> my hand<extra_id_3> my hand with your fork<extra_id_9> why did someone take something _ _?<extra_id_7> to ask me out</s><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "model.branching_strategy = \"standard_beam_search\"\n",
    "outputs = model.generate(input_ids, decoder_input_ids=decoder_input_ids, \n",
    "                         max_length=100,#len(decoder_input_ids[0]), \n",
    "                         output_scores=True,\n",
    "                         num_beams=3,\n",
    "                         num_return_sequences=3,\n",
    "                         return_dict_in_generate=True)\n",
    "# decoded_output = tokenizer.decode(outputs.sequences[2])\n",
    "# print(decoded_output)\n",
    "# decoded_output = tokenizer.decode(outputs.sequences[1])\n",
    "# print(decoded_output)\n",
    "decoded_output = tokenizer.decode(outputs.sequences[1])\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0379, -0.0514, -0.0531])\n",
      "tensor([0.9628, 0.9499, 0.9483])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.sequences_scores)\n",
    "print(outputs.sequences_scores.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 74, 32101])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: we only get `scores` for tokens that was generated during this decoidng, substracting tokens \n",
    "# that were given as input by `decoder_input_ids`. This can explain why \"<pad>\" is always not included.  \n",
    "scores = torch.stack(outputs.scores)\n",
    "scores = scores.transpose(0,1)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.stack(outputs.beam_indices[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (74) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/tmp/ipykernel_38452/338929268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we should use this new functions to get transition probabilities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trs_bs = model.compute_transition_beam_scores(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbeam_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/seq2seq-qasrl/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mcompute_transition_beam_scores\u001b[0;34m(self, sequences, scores, beam_indices, eos_token_id)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mbeam_sequence_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# compute real indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeam_sequence_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# gather scores and run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mtransition_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (74) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# we should use this new functions to get transition probabilities:\n",
    "trs_bs = model.compute_transition_beam_scores(\n",
    "    sequences=outputs.sequences,\n",
    "    scores=outputs.scores, \n",
    "    beam_indices=outputs.beam_indices\n",
    ")\n",
    "# Following https://github.com/huggingface/transformers/issues/15869\n",
    "print(\"Summ:\", torch.sum(trs_bs, dim=1), \"Expected:\", outputs.sequences_scores)\n",
    "print(\"Sum/length:\", torch.sum(trs_bs, dim=1)/len(outputs.beam_indices[0]), \"Expected:\", outputs.sequences_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(outputs.beam_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Manual Posterior Computation\n",
    "\n",
    "We have implemented a computation of \"sum of log probabilites\" in `QASRLSeq2SeqModel.get_sequence_score`.\n",
    "\n",
    "In this section I will use it and try to come up with a method that estimates QA confidence using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import sys, importlib\n",
    "# sys.path.append(\"..\")\n",
    "import seq2seq_model\n",
    "importlib.reload(seq2seq_model)\n",
    "from seq2seq_model import QASRLSeq2SeqModel\n",
    "\n",
    "model_name_or_path = \"kleinay/qanom-seq2seq-model-joint\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = QASRLSeq2SeqModel.from_pretrained(model_name_or_path,config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to `sequence_score`\n",
    "\n",
    "Let's test whether it is similar to the sequence_score returned from generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/kleinay/tmp/ipykernel_38452/1711341721.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_ids = torch.tensor(outputs.sequences)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9717)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generated = model.generate(input_ids, max_length=120, return_dict_in_generate=True)\n",
    "output_ids = torch.tensor(generated.sequences)\n",
    "print(model.get_sequence_score(input_ids, output_ids[0][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[1][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[2][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9717)\n",
      "tensor(0.9618)\n",
      "tensor(0.9594)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_sequence_score(input_ids, output_ids[0][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[1][1:]))\n",
    "print(model.get_sequence_score(input_ids, output_ids[2][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   116,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,  4981, 32090,   113,     3,   834,     3,   834,\n",
       "           808,   424,     3,   834,     3,   834,     3,    58, 32092,    25,\n",
       "         32090,   125,   410,   841,   240,     3,   834,     3,   834,     3,\n",
       "           834,     3,    58, 32092,    82,   609,    28,    39,    21,   157,\n",
       "         32090,   572,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,    12,   987,   140,    91,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [    0,   116,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,  4981, 32090,   113,     3,   834,     3,   834,\n",
       "           808,   424,     3,   834,     3,   834,     3,    58, 32092,    25,\n",
       "         32090,   125,   410,   841,   240,     3,   834,     3,   834,     3,\n",
       "           834,     3,    58, 32092,    82,   609, 32096,    82,   609,    28,\n",
       "            39,    21,   157, 32090,   572,   410,   841,   240,   424,     3,\n",
       "           834,     3,   834,     3,    58, 32092,    12,   987,   140,    91,\n",
       "             1,     0,     0,     0,     0],\n",
       "        [    0,   116,   410,   841,   240,   424,     3,   834,     3,   834,\n",
       "             3,    58, 32092,  4981, 32090,   113,     3,   834,     3,   834,\n",
       "           808,   424,     3,   834,     3,   834,     3,    58, 32092,    25,\n",
       "         32090,   125,   410,   841,   240,     3,   834,     3,   834,     3,\n",
       "           834,     3,    58, 32092,    82,   609,    28,    39,    21,   157,\n",
       "         32096,    82,   609,    28,    39,    21,   157, 32090,   572,   410,\n",
       "           841,   240,   424,     3,   834,     3,   834,     3,    58, 32092,\n",
       "            12,   987,   140,    91,     1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating QA confidence\n",
    "\n",
    "How should we assess different confidence metrics?\n",
    "\n",
    "* We can compute correlations (T test?) of confidence with accuracy (precision)\n",
    "* At [this paper](https://aclanthology.org/W19-8671.pdf), also about confidence in seq2seq generation, they measure the percentage of errors within the 20%/10% of test samples getting the lowest condifence scores. E.g.,for MT, 10% least-confidence finds 17.66% of errors (the consider this a positive result).  In ASR, 10% -> 23.3% of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will write a generic evaluation function that gets a DataFrame and adds a \"confidence\" column \n",
    "# by a certain `confidence_func`. Then, we will run evaluation on lowest 20% to see if precision is significantly lower.\n",
    "# `confidence_func` would take an instance-level DataFrame with QAs of a single predicate.\n",
    "\n",
    "def evaluate_confidence(predictions_df: pd.DataFrame, \n",
    "                        confidence_func: Callable[[pd.DataFrame], pd.DataFrame]):\n",
    "    instance_dfs_with_confidence = []\n",
    "    for _, instance_df in predictions_df.groupby(['qasrl_id', 'verb_idx']):\n",
    "        with_confidence = confidence_func(instance_df)\n",
    "        instance_dfs_with_confidence.append(with_confidence)\n",
    "    df = pd.concat(instance_dfs_with_confidence, ignore_index=True)\n",
    "    # df is same is `predictions_df`, with a new \"confidence\" column.\n",
    "    \n",
    "    # Mark least-confidence QAs\n",
    "    confidence_array = df[\"confidence\"].to_numpy()\n",
    "    percentile = 20\n",
    "    threshold_confidence = np.percentile(confidence_array, percentile)\n",
    "    df[\"is_low_confidence\"] = df[\"confidence\"] <= threshold_confidence \n",
    "    # evalute by \"is_low_confidence\"\n",
    "    evaluate_precision_by_column(df, \"is_low_confidence\", take=True)\n",
    "    \n",
    "    # More Comprehessive assessment of confidence function - \n",
    "    # split data to same-size buckets (e.g. `n_buckets=4` for quartiles)\n",
    "    n_buckets = 4\n",
    "    percentages = list(range(int(100/n_buckets), 100, int(100/n_buckets)))\n",
    "    percentiles = np.percentile(confidence_array, percentages)\n",
    "    confidence_bucket = np.digitize(confidence_array, percentiles) \n",
    "    df[\"confidence_bucket\"] = confidence_bucket   \n",
    "    prec_by_confidence_bucket = evaluate_precision_by_column(df, \"confidence_bucket\")\n",
    "    # plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    uas = [prec_by_confidence_bucket[buck][0] for buck in range(n_buckets)]\n",
    "    las = [prec_by_confidence_bucket[buck][1] for buck in range(n_buckets)]\n",
    "    ax.bar(percentages + [100], uas, color='b')\n",
    "    ax.bar(percentages + [100], las, color='g')\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Functions\n",
    "\n",
    "Each should take an instance-level DataFrame with QAs of a single predicate,\n",
    "and return the same DataFrame with additional \"confidence\" columns.\n",
    "\n",
    "\tIdeas for confidence computations:\n",
    "\t\t1. Posterior Probability baselines:\n",
    "\t\t\ta) Min of QA tokens\n",
    "\t\t\tb) Mean of QA tokens\n",
    "\t\t2. Get score when feeding the QA to decoder as standalone sequence\n",
    "\t\t3. A baseline - score-diff: score(sequence) - score(sequence \\ {QA})\n",
    "\t\t4. Mean of \"score-diff\" for all permutations\n",
    "\t\t5. Score of \"generate another QA\" decision - p(first token of QA) / p(EOS @ first token of QA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import get_markers_for_model\n",
    "special_tokens = get_markers_for_model(is_t5_model=True)\n",
    "def agg_of_QA_token_posteriors(qa_df: pd.DataFrame, agg_func) -> pd.DataFrame:\n",
    "    # construct original output sequence\n",
    "    answers = [a.replace(\"~!~\", special_tokens.separator_output_answers) for a in qa_df.answer]\n",
    "    raw_questions = qa_df.raw_question.tolist()\n",
    "    output_seq = special_tokens.separator_output_pairs.join([f\"{q}{special_tokens.separator_output_question_answer}{ans}\"\n",
    "                                                             for q,ans in zip(raw_questions, answers)])\n",
    "    output_seq = tokenizer.pad_token + output_seq + tokenizer.eos_token\n",
    "    # get token posteriors for all sequence\n",
    "    \n",
    "    # split by QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>predicted output</th>\n",
       "      <th>gold output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parse: -LRB- -LRB- WN -RRB- -RRB- How often do...</td>\n",
       "      <td>&lt;pad&gt; who _ _ gets something _ _?&lt;extra_id_7&gt; ...</td>\n",
       "      <td>who _ _ gets _ _ somewhere?&lt;extra_id_7&gt; the Gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parse: -LRB- -LRB- WN -RRB- -RRB- I was&lt;extra_...</td>\n",
       "      <td>&lt;pad&gt; who was _ looking _ _ somewhere?&lt;extra_i...</td>\n",
       "      <td>who was _ looking _ around _?&lt;extra_id_7&gt; I&lt;ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parse: -LRB- -LRB- WN -RRB- -RRB- That&lt;extra_i...</td>\n",
       "      <td>&lt;pad&gt; what _ _ brings someone _ somewhere?&lt;ext...</td>\n",
       "      <td>what _ _ brings someone to something?&lt;extra_id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parse: -LRB- -LRB- Wikinews -RRB- -RRB- When a...</td>\n",
       "      <td>&lt;pad&gt; who _ _ started something _ _?&lt;extra_id_...</td>\n",
       "      <td>who _ _ started something _ _?&lt;extra_id_7&gt; Duc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parse: -LSB-... -RSB- She may have&lt;extra_id_10...</td>\n",
       "      <td>&lt;pad&gt; who might _ divided something _ _?&lt;extra...</td>\n",
       "      <td>who _ _ divided something _ _?&lt;extra_id_7&gt; She...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>parse: With the three-cylinder compound arrang...</td>\n",
       "      <td>&lt;pad&gt; what was _ set _ _ _?&lt;extra_id_7&gt; the LP...</td>\n",
       "      <td>where was something set _ at something?&lt;extra_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>parse: With two-cylinder compounds&lt;extra_id_10...</td>\n",
       "      <td>&lt;pad&gt; what is _ used _ _ _?&lt;extra_id_7&gt; two-cy...</td>\n",
       "      <td>what is _ used _ in something?&lt;extra_id_7&gt; two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>parse: Younger women today are far more likely...</td>\n",
       "      <td>&lt;pad&gt; who _ _ completed something _ _?&lt;extra_i...</td>\n",
       "      <td>who might _ completed something _ _?&lt;extra_id_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>parse:`` Benjamin Franklin... urged Voltaire t...</td>\n",
       "      <td>&lt;pad&gt; who _ _ agreed _ _ _?&lt;extra_id_7&gt; Voltai...</td>\n",
       "      <td>why did someone agree _ to something?&lt;extra_id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>parse:`` He predestined us to adoption as sons...</td>\n",
       "      <td>&lt;pad&gt; what is _ according _ to something?&lt;extr...</td>\n",
       "      <td>what was _ according _ to something?&lt;extra_id_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  \\\n",
       "0    parse: -LRB- -LRB- WN -RRB- -RRB- How often do...   \n",
       "1    parse: -LRB- -LRB- WN -RRB- -RRB- I was<extra_...   \n",
       "2    parse: -LRB- -LRB- WN -RRB- -RRB- That<extra_i...   \n",
       "3    parse: -LRB- -LRB- Wikinews -RRB- -RRB- When a...   \n",
       "4    parse: -LSB-... -RSB- She may have<extra_id_10...   \n",
       "..                                                 ...   \n",
       "995  parse: With the three-cylinder compound arrang...   \n",
       "996  parse: With two-cylinder compounds<extra_id_10...   \n",
       "997  parse: Younger women today are far more likely...   \n",
       "998  parse:`` Benjamin Franklin... urged Voltaire t...   \n",
       "999  parse:`` He predestined us to adoption as sons...   \n",
       "\n",
       "                                      predicted output  \\\n",
       "0    <pad> who _ _ gets something _ _?<extra_id_7> ...   \n",
       "1    <pad> who was _ looking _ _ somewhere?<extra_i...   \n",
       "2    <pad> what _ _ brings someone _ somewhere?<ext...   \n",
       "3    <pad> who _ _ started something _ _?<extra_id_...   \n",
       "4    <pad> who might _ divided something _ _?<extra...   \n",
       "..                                                 ...   \n",
       "995  <pad> what was _ set _ _ _?<extra_id_7> the LP...   \n",
       "996  <pad> what is _ used _ _ _?<extra_id_7> two-cy...   \n",
       "997  <pad> who _ _ completed something _ _?<extra_i...   \n",
       "998  <pad> who _ _ agreed _ _ _?<extra_id_7> Voltai...   \n",
       "999  <pad> what is _ according _ to something?<extr...   \n",
       "\n",
       "                                           gold output  \n",
       "0    who _ _ gets _ _ somewhere?<extra_id_7> the Gl...  \n",
       "1    who was _ looking _ around _?<extra_id_7> I<ex...  \n",
       "2    what _ _ brings someone to something?<extra_id...  \n",
       "3    who _ _ started something _ _?<extra_id_7> Duc...  \n",
       "4    who _ _ divided something _ _?<extra_id_7> She...  \n",
       "..                                                 ...  \n",
       "995  where was something set _ at something?<extra_...  \n",
       "996  what is _ used _ in something?<extra_id_7> two...  \n",
       "997  who might _ completed something _ _?<extra_id_...  \n",
       "998  why did someone agree _ to something?<extra_id...  \n",
       "999  what was _ according _ to something?<extra_id_...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "model_dir = \"../trained_models/t5_qanom-joint-23.03.22\"\n",
    "prediction_df = pd.read_csv(model_dir + \"/generated_predictions.csv\")\n",
    "raw_prediction_df = pd.read_csv(model_dir + \"/raw_generated_predictions.csv\")\n",
    "\n",
    "# Counter([len(a.split(\"~!~\")) for a in prediction_df.answer])\n",
    "raw_prediction_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efc159d1a32205ec9db73f14cd17171e4e15bf26729f2bc17d41c8a634b0d97c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('seq2seq-qasrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
